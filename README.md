# Cross-Attention CNN-BiLSTM
<!-- /TOC -->
### Cross-Attention Bidirectional CNN-LSTM Network for text classification

#####  Base Contextual-Attention BiLSTM
<img src="img/LSTM.png" align="center" width="450"></p>

##### Sequential CNN-LSTM
<img src="img/CNN-LSTM-S.png" align="center" width="450"></p>

##### Parallel Cross-Attention CNN-LSTM
<img src="img/CNN-LSTM-P.png" align="center" width="450"></p>

##### Word Embedding Space
<img src="img/emb.gif" align="center" width="450"></p>

### Visualized Attention weight map </p>
<p align="center"><img src="img/sample40.png" align="center" width="400" height="225">
<img src="img/sample57.png" align="center" width="400" height="225"></p>
<p align="center"><img src="img/sample77.png" align="center" width="400" height="225">
<img src="img/sample135.png" align="center" width="400" height="225"></p>

References: </p>
[LSTM with Attention by using Context Vector](https://github.com/gentaiscool/lstm-attention)</p>
[Character-level Convolutional Networks for Text Classification](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf)
